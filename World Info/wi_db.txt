"concedo,kobold,koboldcpp,kcpp,ggml,gguf,wiki,help":"KoboldCpp=AI text-generation software for GGML and GGUF models for the KoboldAI Community, made by Concedo. Forked from llama.cpp. Single exe file. Powers ConcedoBot. Get help at https://github.com/LostRuins/koboldcpp/wiki",
    "henk,united,kobold":"Henky=Admin of KoboldAI discord server, manages KoboldAI United, an earlier text-generation software at https://github.com/henk717/KoboldAI",
    "concedo,koboldcpp,lite":"Concedo=Programmer of KoboldCpp, Kobold Lite, and ConcedoBot, also known as LostRuins",
    "lite,frontend":"Kobold Lite=Lightweight WebUI for text-generation at https://lite.koboldai.net",
    "horde,db0":"AI Horde=Crowdsourced distributed cluster of image and text generation servers, made by db0",
    "occam,gpu,clblast,vulkan,opencl":"Occam=KoboldAI User who is a Linux and Vulkan enthusiast",
    "pyro,garg,teapot":"Pyroserenus,Gargamel,Askmyteapot=Volunteers running AI Horde software.\nPyroserenus=KoboldAI User who makes tutorial guides",
    "ooba":"Oobabooga=Competitor's text-generation software,alternative to KoboldAI",
    "silly,tavern,cohee,ross":"SillyTavern=Fancy chat frontend for language models made by Cohee and RossAscends",
    "train,finetune":"LLMs can be finetuned with Axolotl or Hugginface trainer",
    "yellowrose":"YellowRose=Maintainer of KoboldCpp ROCm fork for AMD devices",
    "chatml,alpaca":"ChatML=A poor instruct prompt format. Use Alpaca format instead",
    "hades":"Hades Star=A game concedo used to play",
    "kalomaze,minp,min_p,min-p":"Kalomaze=KoboldAI User that created the Min-P sampler, a new alternative to Top-P",
    "lisa,dave":"Lisa Macintosh=KoboldAI User that hangs around KoboldAI discord basement with Dave",
    "seeker,erebus,nerys,nerybus":"Mr. Seeker=User that created older KoboldAI finetunes like Erebus and Nerys",
    "noli,ecila":"Ecila=Chatbot created by the user Evil Noli, a KoboldAI User",
    "tiefighter,model,bot":"Tiefighter=A merged language model made by Henky and used by ConcedoBot",
    "xzuyn,empty headed":"Empty Headed=KoboldAI User known as xzuyn, finetunes and quantizes models, shitposts memes",
    "lishde":"Lishde=A user swimming in pasta sauce",
    "gelukumlg,ripgel,rip gel,shizuna":"gelukuMLG=KoboldAI User known as Rip Gel, plays minecraft, friends with Shizuna",
    "lyrcaxis":"Lyrcaxis=KoboldAI User who made the aesthetic UI mode for Kobold Lite",
    "ycros":"Ycros=KoboldAI User, a programmer who tinkers around",
    "elinas":"Elinas=Another chatbot maker from KoboldAI",
    "dampf":"Dampf=KoboldAI User, Loves nvidia tensor cores and blast mode",
    "vali,tkinter,customtk":"Vali=KoboldAI User, made the custometkinter UI for KoboldCpp, now making his own app",
    "alpin,aphrodite,pygmalion":"Aphrodite=Alternative backend created by Alpin for the pygmalion community.\nPygmalion=A chatbot model",
    "openai,chatgpt":"ChatGPT=Inferior language model by OpenAI, very censored",
    "aifanboy":"aifanboy=KoboldAI User, also known as DL, hangs around discord",
    "lightsaveus":"LightSaveUs=KoboldAI User, made sampler presets for Kobold United long ago",
    "agnai,sceuick":"Agnaistic=An obscure third party chat UI made by sceuick",
    "alsy,aisy":"AIsy=A cheerful chatbot run by Henky",
    "rwkv,blinkdl":"RWKV=An alternative LLM architecture proposed by BlinkDL",
    "niko,nail":"Niko and Nail are small, red kobolds.",
    "gantian":"Gantian=Original founder of KoboldAI",

    "models,llama,formats,gguf":"KoboldCpp supports many GGML and GGUF model formats besides LLAMA, check the wiki.",
    "quant,q4,q2k,q6,q8":"q4_0,q2k,q6k,q8_0=Model quantizations of different file sizes for different qualities.",
    "gpulayers,layers":"use --gpulayers to control the number of model layers offloaded to GPU.",
    "gguf,download,huggingface":"Download GGUF models from Huggingface. Download KoboldCpp from Concedo's GitHub https://github.com/LostRuins/koboldcpp",
    "clblas,cublas":"Accelerate GPU inference with CLBlast or CuBLAS using --useclblast or --usecublas",
    "build,compile":"After downloading, KoboldCpp can be built with the provided makefile and flags such as make LLAMA_CUBLAS=1, check the wiki.",
    "android,termux,mobile":"Check out the Termux guide for Android on the KoboldCpp wiki, or Kobold Lite.",
    "colab":"Colab runs KoboldCpp on Google Cloud. Link=https://koboldai.org/colabcpp",
    "vram":"Estimating VRAM usage is not easy. Trial and error required.",
    "blasbatchsize":"--blasbatchsize controls the size per text batch sent for processing.",
    "port,localhost":"KoboldCpp runs on localhost at port 5001 by default. This can be changed with --port",
    "stream,sse":"KoboldCpp supports polled streaming and SSE streaming. Check the wiki for info",
    "thread":"Need trial and error to determine number of threads to use with Kobold. Use --threads and --blasthreads. For full offload, use 1 thread, otherwise, use CPU core count.",
    "top-a,top_a":"Top-A is a kobold exclusive alternative sampling method.",
    "mirostat":"Mirostat is an alternative sampling method.",
    "config,kcpps":".kcpps files are configuration files that store KoboldCpp launcher preferences and settings. You can save and load them into the GUI, or run them directly with the --config flag.",
    "multiuser":"--multiuser mode allows multiple people to share a single KoboldCpp instance, connecting different devices to a common endpoint and handles queues automatically.",
    "tunnel,cloudflare,remote":"Run the Remote-Link.cmd or use --remotetunnel to create a TryCloudFlare remote tunnel with a public URL.",
    "onready":"--onready defined a post launch command for KoboldCpp. Check the wiki for more info.",
    "smartcontext,smart context":"--smartcontext reserves a portion of total context space (about 50%) to use as a buffer, reducing processing at the cost of a reduced max context.",
    "contextshift,noshift,context shift":"Context Shifting is a better version of Smart Context that only works for GGUF models. This feature utilizes KV cache shifting to automatically remove old tokens from context and add new ones without requiring any reprocessing. Disable with --noshift",
    "contextsize,context size,ctx size,max context":"For longer prompts, use --contextsize to set max context size you want, such as --contextsize 4096 for a 4K context. Also set it in the UI.",
    "rope config,ropeconfig,rope base,rope scal":"RoPE scaling (via --ropeconfig) is a novel technique capable of extending the useful context of existing language models without finetuning.",
    "mmq":"mmq can be added to --usecublas to use quantized matrix multiplication in CUDA during prompt processing, instead of using cuBLAS for matrix multiplication, using less VRAM.",
    "unbant,unban t,eos":"Language models will use a special EOS (End-Of-Stream) token to indicate when they have finished responding.",
    "api,documentation,docs":"API documentation for koboldcpp can be found on the koboldcpp wiki https://github.com/LostRuins/koboldcpp/wiki",
    "source code":"Source code for Kobold is on GitHub. Source code for ConcedoBot is at https://github.com/LostRuins/ConcedoBot"